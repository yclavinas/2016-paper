% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}
\usepackage[T1]{fontenc}
\usepackage{indentfirst}
\usepackage[english]{babel}
\usepackage{multirow}

\begin{document}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\doi{10.475/123_4}

% ISBN
%\isbn{123-4567-24-567/08/06}

%Conference
%\conferenceinfo{PLDI '13}{June 16--19, 2013, Seattle, WA, USA}

%\acmPrice{\$15.00}

%
% --- Author Metadata here ---
%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Earthquake Risk Models with Genetic Algorithm Hybridization}

%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Blind Author 1%\titlenote{}\\
       \affaddr{University 1}\\
       \affaddr{Address 1}\\
       \affaddr{Address 1}\\
       \email{email@address.com}
% 2nd. author
\alignauthor
Blind Author 1%\titlenote{}\\
       \affaddr{University 1}\\
       \affaddr{Address 1}\\
       \affaddr{Address 1}\\
       \email{email@address.com}
% 3rd. author
\alignauthor
Blind Author 1%\titlenote{}\\
       \affaddr{University 1}\\
       \affaddr{Address 1}\\
       \affaddr{Address 1}\\
       \email{email@address.com}
%\and  % use '\and' if you need 'another row' of author names
%% 4th. author
%\alignauthor Lawrence P. Leipuner\\
%       \affaddr{Brookhaven Laboratories}\\
%       \affaddr{Brookhaven National Lab}\\
%       \affaddr{P.O. Box 5000}\\
%       \email{lleipuner@researchlabs.org}
%% 5th. author
%\alignauthor Sean Fogarty\\
%       \affaddr{NASA Ames Research Center}\\
%       \affaddr{Moffett Field}\\
%       \affaddr{California 94035}\\
%       \email{fogartys@amesres.org}
%% 6th. author
%\alignauthor Charles Palmer\\
%       \affaddr{Palmer Research Laboratories}\\
%       \affaddr{8600 Datapoint Drive}\\
%       \affaddr{San Antonio, Texas 78229}\\
%       \email{cpalmer@prl.com}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{30 July 1999}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
The GAModel is a method which aims to generate forecasts by using only Evolutionary Computation (EC). However, the GAModel was limited by the very high number of parameters, and the subsequent large search space. This project main goal is to refine the GAModel ideas, objective to have a greater ability to predict the future behavior of groups of earthquakes events by overcoming this limitations.  % It is based purely on a standard Genetic Algorithm (GA) with real valued genome, where each allele corresponds to a bin in the forecast model. 

This document sumamarizes the GAModel and proposes three new method based on the GAModel. The first method is called the ReducedGAModel. By this, we expecte to reduce the impact of the amount of earthquakes parameters and the size of the search space generating a forecast model faster. For that we used a different genome representation than the representation used in the GAModel. In the GAModel, the genome is a real valued genome, with every gene coresponds to a specific bin in the model. In the ReducedGAModel, the genome is a list of earthquake events locatation. Each element of the genome has a correspondent bin in the model.

We also wanted to do a hybridization, a association of EC and geophysical knowledge, of the GAModel and the ReducedGAModel with empirical laws, such as the modified Omori-Utsu formula. These new methods names are Emp-GAModel and Emp-ReducedGAModel. This hybridization is performed in two phases. The first one, is to obtain forecast models obtained by the GAModel and the ReducedGAModel. After that, in the second phase, these models are all refined by the same group of geophysical formulas. We expect that we will be able to generate not only faster model but more accurated ones, because we will reduce the search space and increase its learning rate with the formulas.


The models generated by these four method were evaluated and compared based on the predictabily experiments framework proposed by the "Collaboratory for the Study of Earthquake Predictability" (CSEP), an international effort to standardize the study and testing of earthquake forecasting models. The experiments were designed to compare 1-year earthquake rate forecasts for four regions in Japan in using the data from the Japan Meteorological Agency (JMA) earthquake catalog.

%The best model was then blablabla smack
\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
%\begin{CCSXML}
%<ccs2012>
% <concept>
%  <concept_id>10010520.10010553.10010562</concept_id>
%  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%  <concept_significance>500</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010575.10010755</concept_id>
%  <concept_desc>Computer systems organization~Redundancy</concept_desc>
%  <concept_significance>300</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010553.10010554</concept_id>
%  <concept_desc>Computer systems organization~Robotics</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
% <concept>
%  <concept_id>10003033.10003083.10003095</concept_id>
%  <concept_desc>Networks~Network reliability</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
%</ccs2012>  
%\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}


%
% End generated code
%

%
%  Use this command to print the description
%
\printccsdesc

% We no longer use \terms command
%\terms{Theory}

%\keywords{Evolutionary Computation, Genetic Algorithms, Forecasting, Earthquakes}


\section{Introduction}\label{intro}
Earthquakes may cause soil rupture or movement, tsunamis and more. They may cause great losses and that can be explicit by some examples, such as the earthquakes in Tohoku (2011) and Nepal(2015). To be able to minimize the consequences of these events, we look to create forecast earthquake occurences models. Hence the characteristics that most influence the earthquakes events may vary both in time and place, these methods should be to adapt their behavior to be able to forecast earthquakes events which reflects well the reality. 

This project aims to obtain a better method, based in improvements to the GAModel~\cite{ecta14}, a statistical method of analysis of earthquakes risk using the Genetic Algorithm technique (GA). Two ideias are proposed for this. The first one, is to change the candidate solution representation. By that, we objective to make the GAModel more specialized, focusing only on areas on which earthquakes happened already in a near past. This will lead to a faster convergence, once the amount of parameters is smaller and consequently, the search space gets smaller. 

Formulated on this idea, we propose the ReducedGAModel. Its genome only has information of areas that already had occurences in the past. This helps the method to converge gets faster, by minimizing the number of parameters the method has to deal with.

The other ideia is based on the assumption that earthquakes cluster in both space and time, and the we want associate the Genetic Algorithm technique (GA) with a some empirical laws, such as the modified Omori law. First, the background intensity (the independent earthquakes or mainshocks), which is a function of the space, is forecasted using the GA. Then, we use some empirical laws to obtain the dependent earthquakes (aftershocks) for a specific time interval.

The Emp-GAModel is the method proposed that incorpores some geophysical knowledge. It is a hybridization of the models generated by the GAModel with the these empirical laws, see Section \ref{Models}.

Finally, there is the Emp-ReducedGAModel. This method is a combination of the two ideias. Therefore, it also performs a hybridization of models with the group of empirical law. Though, for this method, the models are generated by the ReducedGAModel method and not by the GAModel. 

The forecast models produced by those methods and the ones produced by the GAModel were all analyzed using likelihood tests, namely the L-test, the N-test and the R-test, as suggested by Regional Earthquake Likelihood Model (RELM)~\cite{schorlemmer2007earthquake}.

For developing the methods and to be able to compare them we used the earthquake catalog from the Japanese Meteorological Agency (JMA), using event data from 2005 to 2010.  

This paper is organized as: in Section \ref{estadoArte}reviews applications of Evolutionary Computation in the context of seismology research. The next Section,Section \ref{Models}, we give a details of each of the forecast proposed covering the Collaboratory for the Study of Earthquake Predictability (CSEP) framework and the empirical laws. In Section \ref{Tests}, we give the description of the tests proposed in~\cite{Schorlemmer2007}. After that, in \ref{exp}, we define the target areas used for the experiment and the data from the JMA; we clarify the design followed during the experiments and how ew compared the forecast models derived from our methods. Finally, we show the results and conclude this work in~\ref{Results} and \ref{Conclusions}.

%TODO: change text to mine version
%TODO: update info + ECTA
\section{Evolutionary Computation for Earthquake Risk Analysis}\label{estadoArte}
In this section we will briefly discuss some reports of the application of Evolutionary Computation and related method for Earthquake Risk Analysis.

The usage of Evolutionary Computation in the field of earthquake risk models is somewhat sporadic. Zhang and Wang~\cite{Zhang2012} used Genetic Algorithms to fine tune an Artificial Neural Network (ANN) and use this system to produce a forecast model. Zhou and Zu~\cite{Feiyan2014} also proposed a combination of ANN and EC, but their system only forecasts the magnitude parameter of earthquakes. Sadat, in the paper~\cite{sadat2015application}, follows the idea oF Zhou and Zu, aiming to predict the magnitude of the earthquakes in North Iran, but in this case, he used ANN and GA. 

Some sismological models were developed aiming to estimate parameter values by using Evolutionarry Computation. For example, Evolutionary Computation was used to estimate the peak ground acceleration of seismically active areas~\cite{Kermani2009,Cabalar2009,Kerh2010,Kerh2015}. Ramos~\cite{Ramos2011} used Genetic Algorithms to decide the location of sensing stations and Saeidian~\cite{saeidian2016evaluation} made a comparation in performance between the GA and Bees Algorithm to decide which of those techniques would performe better when chosing the location of sensing stations. Nicknam et al.~\cite{Nicknam2010} and Kennett and Sambridge~\cite{Kennett1992} used evolutionary computation to determine the Fault Model parameters of a earthquake.

%TODO: Last part in somewhat bad
\subsection{What is Evolutionary Computation And What are Genetic Algorithms}\label{EC}
	Evolutionary Computation (EC) is concerned with algorithms based on the Darwinian principles of natural selection. It may find, by try trial and error and based on a great amount of data, better solutions for problems that human beings may not find it easy to solve~\cite{koza2003genetic}. That could also be done without any domain knowledge about the problem to be controlled~\cite{Michie94machinelearning}.
	
The main goal of a Genetic Algorithm (GA), a EC technique, is to find approximated solutions in problems of search and optimization. Based on Koza~\cite{koza2003genetic}, GA  are mechanism of search based on natural selection and genetic. They explore historical data to find optimum search points with some performance increment~\cite{Goldberg:1989:GAS:534133}.

	A GA uses those mechanisms to generate solutions to optimization and search problems. The first step is to create an initial population, a group of possible solutions, where each solution is called an individual. Those individuals have its fitness value estimated by a given function and those with greater fitness value are then chosen to reproduction. After some evaluations, we expect to find an optimum solution. 

	Frequently, the initial population is randomly generated once it is common to ignore the main aspects that influence the algorithm performance. In other words, hence it is common to lack domain knowledge, the random population is a good way to start searching for optimum solutions. 

%TODO: Why 1-year?
\section{The Forecast Models Using Genetic Algorithm}\label{Models}

All forecast models proposed in this paper are based in the Collaboratory for the Study of Earthquake Predictability (CSEP) framework. 

Each individual has its own representation of the framework based on different perceptions of what are the best aspects of the framework. 

The population is trained on earthquake event data for a training period, which is anterior to the target test period. After completing the evalution limit, the best individual is chosen to be the final forecast.
 
icio\subsection{1-year Models}
In the CSEP framework, a forecast model uses one common format in the literature,	 which is a gridded rate forecast~\cite{zechar2010evaluating}. For this format a geographical region, during a start date and an end date is divided in sections, the bins. The forecast will estimate the number (and sometimes the magnitude) of earthquakes that happens in this target region, during the target time interval, considered to be of one year for this study~\cite{ecta14}. 

Large and independent earthquakes, also known as mainshocks, are followed by a wave of others earthquakes, the aftershocks~\cite{schorlemmer2010first}. Hence there is no physical measurement to identify mainshocks and its aftershocks~\cite{schorlemmer2010first}, we divided the forecast models in two groups: the ones that only forecasts mainshocks and those that forecast both mainshocks and aftershocks.

\subsection{Genome Representation}\label{genome}
The GAModel each individual represents an entire forecast model~\cite{ecta14}. For the ReducedGAModel and each individual represents a subarea of the forecast model. This subarea is related to earthquakes past events locations only, so the genome size is usually smaller than the one used in the GAModel and the Emp-GAModel. 

The Emp-ReducedGAModel and the Emp-GAModel differs only from the ReducedGAmodel and from the GAmodel, respectively, by the use of equations after the forecast is provided. This means that the theirs genome representation are the same as the GAModel and the ReducedGAModel,correspondingly.

For all methods, the genome is a real valued array X, where each element corresponds to one bin in the desired model (the number of bins n is defined by the problem). Each element $x_i \in X$ takes a value from $[0,1)$. In the initial population, these values are sampled from a uniform distribution. For more details of the genome represatation, please refer to~\cite{ecta14}.
\subsection{Fitness Function}
For the fitness function we used the log-likelihood value. The fittest individual among all the others, is preserved in the next generation, to make the solution of one generation as good as the its last generation.
The bins, a gene (a element) of the genome representation, $b_n$, define the set $\beta$ and $n$ is the size of the set $\beta$:

				\begin{equation} 
					\beta := {b_1,b_2,...,b_n},n = |\beta|
				\end{equation}

The probability values of the model $j$,  expressed by the symbol $\Lambda$, is made of expectations $\lambda_i^j$ by  bin $b_i$. The vector is define as:
				
\begin{equation}
	\Lambda^j = 
\begin{pmatrix}
    \lambda_1^j, 
    \lambda_2^j, 
    \hdots,
    \lambda_i^j
  \end{pmatrix}
  ;\lambda_i^j := \lambda_i^j(b_i),b_i \in \beta
\end{equation}
		
The vector of earthquake quantity expectations is defined as: earthquake by time. The $\Omega$ vector é composed by observations $\omega_i$ per bin  $b_i$, as the  $\Lambda$ vector:

\begin{equation}
\Omega = 
\begin{pmatrix}
    \omega_1,
    \omega_2,
    \hdots,
    \omega_i
  \end{pmatrix}
  ;\omega_i =\omega_i(b_i),b_i \in \beta
\end{equation}

The calculation of the log-likelihood value for the  $\omega_i$ observation with a given expectation $\lambda$ is defined as:


\begin{equation}
	L(\omega_i|\lambda_i^j) = -\lambda_i^j + \omega_i\log\lambda_i^j - \log\omega_i!
\end{equation}

The joint probability is the product of the likelihood of each bin, so the logarithm $L(\Omega|\Lambda^j)$ is the sum of for $L(\omega_i|\lambda_i^j)$ every bin $b_i$:

\begin{equation}\label{log-like}
\begin{split}
	L^j = L(\Omega|\Lambda^j) = \sum_{i=1}^{n}L(\omega_i|\lambda_i^j)  \\
	= \sum_{i=1}^{n} -\lambda_i^j + \omega_i\log\lambda_i^j - \log\omega_i!  
\end{split}
\end{equation}

The fitness function is a coded version of the equation ~\ref{log-like}. It uses the probabilities of the bins of each individual of model for the $\lambda$ values. 

\subsection{Evolutionary Operators}
Both the GAModel and the Emp-GAModel uses a combination of operators made available by the Distributed Evolutionary Algorithms in Python (DEAP)~\cite{DeRainville:2012:DPF:2330784.2330799}. We use the One Point Crossover for the crossover operator, for the mutation operator, the Polynomial Bounded Mutation and for selection, we use Elitism and Tournament selection, with size 3.

For the ReducedGAModel and the Emp-ReducedGAModel, the only different operator is the mutation fuction. We use a simple mutation operator which samples entirely two new valus. The first, is a new real value from [0,1) and the seconde one, a new integer value from [0,X), where X is the maximum size of the genome.



\subsection{Mainshock Models}
The GAModel is purely based on the framework suggested by the CSEP. In it, one forecast is defined as a place for a specfic time and is divided in bins. Each bin represents a geographical interval. The whole target area of study is covered by these bins and represents the $\mu(x,y)$, the background intensity~\cite{zhuang2004analyzing}. In the GAModel, each possible solution is represented as an entire forecast model.

In this context the GAModel is considered as one method to generate space-rate-time forecasts. It also could be described as:

\begin{equation}\label{gamodel}
 \lambda(t,x,y,M|\Upsilon_t) = \mu(x,y)
\end{equation}

where you can denote the number of earthquakes forecast in a bin as $\lambda(t,x,y)$~\cite{zechar2010evaluating} given that $\Upsilon_t$ is the earthquake observation data up to time $t$.% Which leads to $\mu(x,y)$ the background intensity, a function of space, but not of time~\cite{zhuang2004analyzing}.

The ReducedGAModel, which represents the first ideia (see~\ref{intro}), is a method described as the GAModel, but each possible solution represents a fraction of the forecast where we expect to find especific risk areas. The GAModel defines a expected number of earthquakes for every single bin in the target region. That could lead to exhaustive and, sometimes worthless, searches. That is caused by the number of bins in the forecast and also because some in some bins there are no earthquake occurances in the observation data. To minimize this, the ReducedGAModel only define expected number of earthquakes in bins that already had some occurance in the past, giving some sort of guideline to the GA. 

To make it clear, we use the same example as the one used in ~\cite{ecta14}.The "Kanto" region, one of the four areas used in this study, is divided into 2025 bins (a grid of 45x45 squares). Each bin has an area of approximately $25km^2$. The GAModel then calculates an expected number of earthquakes for every bin on a determinated time interval, so the GA searches for good values in 2025 bins. 

The ReducedGAModel will first obtain the position of past occurances and will calculate some expected number of earthquakes for only the bins related to those positions. For example, if there are 10 bins with occurances in Kanto in the last year, it will make the GA search good values for only those 10 bins, leaving the other 2015 bins with the value zero, representing zero occurances. It is important to highlight that in the worst case, it will make the same amount of searches as the GAModel.

% (it is possible that an bin had more the one occurance, therefore that number will drop. 

\subsection{Mainshock+Aftershock Models}
Hence earthquakes cluster in space and inspired by the space-time epidemic-type aftershock sequence (ETAS), the Emp-GAModel, represents the second ideia (see~\ref{intro}) and is described as:

\begin{equation}\label{reducedgamodel}
	\lambda(t,x,y,M|\Upsilon_t) = \mu(x,y)J(M)
\end{equation}

%TODO: adicionar RI nanjo no lugar da F
\begin{equation}\label{emp-model}
 \lambda(t,x,y|\Upsilon_t) = \mu(x,y) + \displaystyle\sum_{t_i \in t} K(M_i)g(t-t_i)F
\end{equation}
%TODO: explicar o Upsilon

The Emp-GAModel uses $\mu(x,y)$ as defined for the GAModel, so it calculates an expected number of earthquakes for every bin in the target region. The Omori law, $g(t)$, which is considered one empirical formula of great success~\cite{zhuang2004analyzing}~\cite{utsu1995centenary}~\cite{omori1895after}, is a power law that relates the earthquake occurance and its magnitude with the decay of aftershocks activity with time. For this approach we used the probabilty density function (pdf) form of the modifed Omori law~\cite{zhuang2004analyzing}:

\begin{equation}\label{omori}
	g(t)= \dfrac{(p-1)}{c(1+ \dfrac{t}{c})^(-p)}
\end{equation}

In the paper~\cite{utsu1995centenary}, Utsu says that most p and c values, for various earthquake data sets fall in the range between 0.9 and 1.4, and between 0.003 and 0.3 days, respectively. These values were based on the Davidon-Fletcher-Powell optimization procedure and used in ETAS~\cite{utsu1995centenary}. For the experiments done for this paper, we choose the values of $1.3$ for $p$ and $0.003$ for $c$, arbitrarily.

We count the number of aftershocks which occur within a month after each main shock. If the constants c and p in the modified Omori formula, Eq. (1) are common in all the sequences, varying the time interval only causes an increase or decrease of the aftershock number by a constant ratio. According to Utsu (1969), the mean p-value in Eq. (1) is 1.3 and the mean c-value is 0.3 days. We set the time interval at one month. If it is too short, the aftershock number becomes too small. On the other hand, if it is too long, we may also count the events due to background activity. When an event is a doublet, we take one-month interval from the second event.

For $K(M_i)$, the total amount of triggered events, we count aftershocks within a calculated aftershock area, $A$, using the formula, where $M_c$ is the magnitude threshold:

\begin{equation}\label{triggered}
 K(M_i) = A\ exp([\alpha(M-M_c)])
\end{equation}

From~\cite{ogata2006space}, states alpha as the inverse of the magnitude of an event, or $magnitide^{-1}$. To obtain A, the following equation from~\cite{yamanaka1990scaling}, was used:

\begin{equation}
A = e^{(1.02M -4)}
\end{equation}

and lastly, the $J(M)$ is a simulation of the event  magnitude by Gutenberg-Richter's Law, using $1$ as the value of $\beta$ ~\cite{helmstetter2003predictability}:

\begin{equation}\label{gut-ritcher}
J(M) = \beta e^{-\beta(M-M_c)}, M \geq M_c
\end{equation}

At last, the Emp-ReducedGAModel is a mix between the two ideias, so it is represented as the Emp-GAModel but its candidates take the same form as the ones in the ReducedGAModel.

%TODO: Move this whole section to exp (where there?)
\section{Tests for evaluating Models}\label{Tests}

In the paper {\it Earthquake Likelihood Model Testing}~\cite{schorlemmer2007earthquake} is proposed some statistical tests that are used in this study to compare and evaluate the forecast models, developed by the The Regional Earthquake Likelihood Models (RELM). These tests are based on the log-likelihood score that compares the probability of the model with the observed events. 

To evalute the data-consistency of the forecast models we used the N-Test, the Number Test,  and the L-Test, or Likelihood Test. These tests fall are significance tests. Therfore, assuming a given forecast model as the null hypothesys, the distribuition of an observable test is simulated. If the observed test statistic falls into the upper or lower tail of this distribuition, the forecast is rejected~\cite{schorlemmer2010first}.

To be able to compare the model that passed the N-Test and the L-test, the R-Test, hypotheses Comparison Test is used. It calculates the relative performance of a model, by comparing the Log-likelihood values between two forecast models.

\subsection{Likelihood Test or L-Test}
 
 The L(ikelihood) Test considers that the likelihood value of the model is consistent with the value obtain with the simulations. The value is calculated by the formula, where $\widehat{L}_k$ is the value of the Log-likelihood of the model {\it j}, in the {\it bin} {\it i} and $\widetilde{L}$ is the value of the Log-likelihood of the simulation {\it j} in the {\it bin} {\it q}: 


\begin{equation}
\gamma^{j}_{q} = \frac{\left| \left\{ \widehat{L}^j_k | \widehat{L}^j_k \leq \widetilde{L}^j_q, \widehat{L}^j_k \in \widehat{L}^j, \widetilde{L}^j_q \in \widetilde{L}^j  \right\} \right|}  {|\widehat{L}^j|}
\end{equation}

The analisys of the results can be splited into 3 categories, as follows:

\begin{enumerate}
\item Case 1: $\gamma^{j}$ is a low value, or in other words, the Log-likelihood of the model is lower then most of the Log-likelihood of the simulations. In this case, the model is rejected.
\item Case 2: $\gamma^{j}$ falls near the half of the values obtained from the simluations and is consistent with the data.
\item Case 3: $\gamma^{j}$ is high. This means that the Log-likelihood of the data da is higher that the Log-likelihood of the model and no conclusion can be made what so ever.
\end{enumerate}


It is important to highlight that no model should be reject in case 3, if based only on the L-Test. In this case the consistency can or cannot be real, therefore these model should be tested by the N-Test so that further conclusions can be done.

\subsection{Number test or N-Test}
The N(umber)-Test also analises the consistency of the model, but here, it compares the number os observations with the number of events of the simulations. This test is necessary to supply the underpredicting problem, which may pass unnotecied by the L-Test. 

This mesures is estimated by the fraction of the total number of observations by the total number of observations of the model.

As the L-test, if the number of events falls near the half of the values of the distruition, then the model is consistent with the observation, nor estimating too much events nor few of them.

\subsection{Hypotheses Comparison Test or R-Test}

The Hypotheses Comparison, or the R(atio)-Test, compares two forecast models against themselves. The log-likelihood is calculted for both models and then the difference between, the observed likelihood ratio, this value indicates which one of the model better fits the observations. 

The likelihood ratio is calculated for each simulated catalog. If the fraction of simulated likelihood ratios less than the observed likelihood ratio is very small, the model is reject.  To make this test impartial, not given an advantage to any model, this procedure is applied symmetrically~\cite{schorlemmer2010first}.


\subsection{Evaluation}\label{eval}
The evaluation process is made as follow: First, the data-consitency is tested by the L-Test and the R-test. If the model passes these tests, meaning that it was not rejected by them, it is compared with other forecast models, which also were not reject, with the R-Test. The model that best fits the R-Test is then chose as the best model~\cite{schorlemmer2007earthquake}.

%data, design, comparacao, regions, year(s)
%para o só ga ,boxplot e t-test tem que ver se tem vantagem
%para com equations usar os testes
\section{Experiments}\label{exp}
%Tuned configuration: A parameter configuration tuned for the smaller budget of 500 evaluations used in this experiment. The tuned parameter values for each algorithm were calculated using the Sequential Model-based Algorithm Configuration method (SMAC) [7].

To analyze the performance of the forecasts generated by the GAModel, the ReducedGAModel, the Emp-GAModel and finally the Emp-ReducedGAModel, we used the evalution method proposed by~\cite{schorlemmer2007earthquake} and described in section~\ref{eval}.

Objecting a better understand of the patterns that most influence the earthquakes events and also to be able to determine the qualities of those forecasts, the data of the JMA catalog was divided into four groups. Each group constituent only of earthquakes that happened in a specific time interval for a given area of Japan. The experimental data will be described in details subsequently.

\subsection{Experimental Data}
The data used in these experiments comes from the Japan Meteorological Agency's (JMA) catalog. It is a list of earthquakes events which took place in Japan from 2000 to 2013. Each event is characterized by some typical earhtquake information such as magnitude, latitude, longitude, and depth.

For the experiments we consider events with magnitude above 5.0 which happened in four specific areas of Japan, during the year of 2010. Those areas (Kanto, Tohoku, Kansai and East Japan) represent different earthquake attributes and could lead to more information about the power of the forecasts and/or its pitfalls. Kanto, Touhoku and Kansai contain mainly inland earthquakes, which are considered to follow more stable patterns. East Japan includes also many off-shore earthquakes. For more information about the four regions as well as a map which locates them in Japan, please refer to~\cite{ecta14}.


%
%Kanto: Kanto is the area at and nearby Tokyo. There is a large amount of seismic activity in the target period. In this study, we define this region as starting at N34.8,W138.8, with 2025 bins ar- ranged in a 45x45 grid. Each bin corresponds to a 5km2 square.
%
%Kansai: The Kansai area includes Kyoto, Osaka, Kobe and nearby cities. This area shows a rela- tively lower amount of seismic activity in the pe- riod considered. We define this region as start- ing at N34,W134.5, with 1600 bins arranged in a 40x40 grid. Each bin corresponds to a 5km2 square.
%
%Touhoku: The Touhoku region is defined as the northern provinces of the main island. It shows some clusters of seismic activity during the pe- riod studied. We define this region as starting at N37.8,W139.8, with 800 bins arranged in a 40x20 grid. Each bin corresponds to a 10km2 square.
%
%East Japan: This area corresponds to Japan’s north-eastern coast. It includes both inland and off-shore events, which makes it more difficult to forecast. It also includes the location of the M9 earthquake of 2011. We define this region as starting at N37,W140, with 1600 bins arranged in a 40x40 grid. Each bin corresponds to a 10km2 square.

\subsection{Experimental Design}
To compare the performance of the forecast models, we execute a simulation experiment on ??? scenarios. Each scneario is defined by a region for a 1-year interval, starting in Jan/01 and endding in Dec/31. To build one scenario, one region is chosen from the set of regions (Kanto, Tohoku, East Japan and Kansai) for a determinated year. To train the methods, we use 5 years of prior data, for example, to train the 2010 1-year GAModel Kanto region, the data used is from the events which occurred in the years of 2006, 2007, 2008 and 2009 in Kanto.

To The results of the forecasts io, we used two approaches. The first one is based on the evaluation method proposed by~\cite{Schorlemmer2007} and used in~\cite{schorlemmer2010first}. As described in~\ref{eval}, the forecasts were first analysed using the L- and N- test and ??????  was chosen by the R-test as the recommended one.

The second approach is to using the Log-likelihood values obtained by each of the forecast for each scenario. The log likelihood indicates how close the forecast is to the test data, in terms of location and quantity of earthquakes~\cite{ecta14}.

All forecast models here presented are stochastic methods. Therefore, to be able to compare test the statistical significance of the results, we run each forecast model 10 times. That means that all results showed are the mean of these 10 executions.

\subsection{Parameter Tuning}
%All 4 algorithms were tuned using the Sequential Model- based Algorithm Configuration, SMAC [7]. 

Parameter tunning is a commonly practiced approach which aims to find optimum parameters values before the run of a algorithm. Later this algorithm is executed with these values, which remain fixed during the run~\cite{eiben2011parameter}.

%review this
To find a good set of parameters for the all the forecast models algorithms, the pySMAC, a Python wrapper for the hyperparameter optimization tool Sequential Model- based Algorithm Configuration (SMAC)~\cite{hutter2010sequential}, was used.

SMAC is an algorithm configurator takes as input an algorithm executable, a formal description of the parameters for the algorithm, and a set of training problem instances. It searches the space of possible parameter values by generating a candidate configuration and evaluating the configuration on the set of the training instances~\cite{aranha2015optimization}. 

The configuration which best fits each one of the forecast models algorithm is then chosen to be used as the set of values for the its algorithms run. For all algorithms present here, the default control parameter values (which are based on the values applied in the GAModel~\cite{ecta14}), the ranges for the parameters (arbitrarily chosen), as well as the values found by SMAC, are shown in Table~\ref{smac-par} for GAModel and Emp-GAmodel and in Table~\ref{smac-par2} for ReducedGAModel and Emp-ReducedGAModel.

EXPLICAR A QUESTAO DA EXPLICACAO DOS PARAMETROS $N_GEN * POP = CONST$


\begin{table}[!h]
  \begin{center}
  \caption{The default control parameter values and the best set of parameters found by tunning the algorithm with SMAC for the ReducedGAModel and the Emp-ReducedGAModel}
\begin{tabular}{ |c|c|c|c|c| }
\hline
%\multicolumn{5}{ |c| }{SMAC configuration and values for GAModel and Emp-GAModel} \\
\hline
\multirow{5}{*}{Kanto}
 & Parameter & Range & Default & Tuned \\
 \cline{2-5}
 & Crossover Chance & [0,1] & 0.9 & 0.9\\
 & Mutation Chance & [0,1] & 0.1 & 0.1\\
 & Population Size & N/A & 500 &\\
 & Generation Number & [50,250] & 100 & 100 \\ \hline
\multirow{4}{*}{Kansai}
 & Parameter & Range & Default & Tuned \\
 \cline{2-5}
 & Crossover Chance & [0,1] & 0.9 & 0.9\\
 & Mutation Chance & [0,1] & 0.1 & 0.1\\
 & Population Size & N/A & 500 &\\\ 
 & Generation Number & [50,250] & 100 & 100 \\ \hline
\multirow{4}{*}{Tohoku}
 & Parameter & Range & Default & Tuned \\
 \cline{2-5}
 & Crossover Chance & [0,1] & 0.9 & 0.9\\
 & Mutation Chance & [0,1] & 0.1 & 0.1\\
 & Population Size & N/A & 500 &\\
 & Generation Number & [50,250] & 100 & 100 \\ \hline
\multirow{4}{*}{East Japan}
 & Parameter & Range & Default & Tuned \\
 \cline{2-5}
 & Crossover Chance & [0,1] & 0.9 & 0.8826\\
 & Mutation Chance & [0,1] & 0.1 & 0.1724\\
 & Population Size & N/A & 500 &\\
 & Generation Number & [50,250] & 100 & 106 \\ \hline
\hline
\end{tabular}
  \end{center}
  \label{smac-par2}
\end{table}

\begin{table}[!h]
  \begin{center}
  \caption{The default control parameter values and the best set of parameters found by tunning the algorithm with SMAC for the ReducedGAModel and the Emp-ReducedGAModel}
\begin{tabular}{ |c|c|c|c|c| }
\hline
%\multicolumn{5}{ |c| }{SMAC configuration and values for GAModel and Emp-GAModel} \\
\hline
\multirow{5}{*}{Kanto}
 & Parameter & Range & Default & Tuned \\
 \cline{2-5}
 & Crossover Chance & [0,1] & 0.9 & 0.9\\
 & Mutation Chance & [0,1] & 0.1 & 0.1\\
 & Population Size & N/A & 500 &\\
 & Generation Number & [50,250] & 100 & 100 \\ \hline
\multirow{4}{*}{Kansai}
 & Parameter & Range & Default & Tuned \\
 \cline{2-5}
 & Crossover Chance & [0,1] & 0.9 & 0.9\\
 & Mutation Chance & [0,1] & 0.1 & 0.1\\
 & Population Size & N/A & 500 &\\\ 
 & Generation Number & [50,250] & 100 & 100 \\ \hline
\multirow{4}{*}{Tohoku}
 & Parameter & Range & Default & Tuned \\
 \cline{2-5}
 & Crossover Chance & [0,1] & 0.9 & 0.9\\
 & Mutation Chance & [0,1] & 0.1 & 0.1\\
 & Population Size & N/A & 500 &\\
 & Generation Number & [50,250] & 100 & 100 \\ \hline
\multirow{4}{*}{East Japan}
 & Parameter & Range & Default & Tuned \\
 \cline{2-5}
 & Crossover Chance & [0,1] & 0.9 & 0.8826\\
 & Mutation Chance & [0,1] & 0.1 & 0.1724\\
 & Population Size & N/A & 500 &\\
 & Generation Number & [50,250] & 100 & 106 \\ \hline
\hline
\end{tabular}
  \end{center}
  \label{smac-par2}
\end{table}


%5 https://github.com/automl/pysmac

%TODO: tabelas, imagens, add
%TODO: Discussion of the results, what should i do? is it good?...?
\section{Results}\label{Results}

%TODO: add future work with Gabriels
%TODO: this section should be a revision of all
\section{Conclusions}\label{Conclusions}
%\end{document}  % This is where a 'short' article might terminate

%ACKNOWLEDGMENTS are optional
\section*{Acknowledgments}
Bogdan, Zechar, Zhuang

???

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{bib}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns

\end{document}
